# agents/cx_analyst.py

import json
import numpy as np
import pandas as pd
import networkx as nx

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import MiniBatchKMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
from .utils import get_sentiment_analyzer
from .utils import get_openai_client
from scipy.sparse import csr_matrix # í¬ì†Œ í–‰ë ¬ ë³€í™˜ ì‹œ í•„ìš”
from collections import defaultdict 
from sklearn.decomposition import PCA, LatentDirichletAllocation
from scipy.sparse import csr_matrix # ğŸš¨ ì¶”ê°€: csr_matrix ì„í¬íŠ¸
import community as co

# --- ë‚´ë¶€ í—¬í¼(ë³´ì¡°) í•¨ìˆ˜ë“¤ ---
def _get_sentiment_score(text: str) -> float:
    """í…ìŠ¤íŠ¸ì˜ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (ë™ê¸° ë²„ì „)"""
    analyzer = get_sentiment_analyzer() # ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ await ì—†ìŒ
    if analyzer is None:
        print("âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°ì„± ì ìˆ˜ ê³„ì‚° ë¶ˆê°€.")
        return 0.0 # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ì ìˆ˜ ë°˜í™˜
    
    try:
        # Hugging Face pipeline ê²°ê³¼ëŠ” ë¦¬ìŠ¤íŠ¸ [{label: 'LABEL_0', score: 0.99}] í˜•íƒœ
        # bert-nsmc ëª¨ë¸ì€ LABEL_0 (ë¶€ì •)ê³¼ LABEL_1 (ê¸ì •)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        result = analyzer(text[:512])[0] # ëª¨ë¸ ì…ë ¥ ê¸¸ì´ ì œí•œ (ì˜ˆ: 512)
        print(result)
        # LABEL_1 (ê¸ì •)ì€ ê¸ì • ì ìˆ˜, LABEL_0 (ë¶€ì •)ì€ ë¶€ì • ì ìˆ˜ë¡œ ë§¤í•‘
        if result['label'] == 'positive': # ê¸ì •
            return float(result['score'])
        elif result['label'] == 'negative': # ë¶€ì •
            return -float(result['score'])
        else: # ì¤‘ë¦½ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš° (ì—†ì„ ê°€ëŠ¥ì„± ë†’ìŒ)
            return 0.0
    except Exception as e:
        print(f"ê°ì„± ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0.0 # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ ì ìˆ˜ ë°˜í™˜

def _get_top_keywords(feature_names, topic_components, n_top_words):
    """
    LDA í† í”½ ëª¨ë¸ì˜ ì»´í¬ë„ŒíŠ¸(ë‹¨ì–´-í† í”½ ë¶„í¬)ì—ì„œ ê° í† í”½ë³„ ìƒìœ„ Nê°œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Args:
        feature_names (list): TF-IDF ë²¡í„°ë¼ì´ì €ì˜ í”¼ì²˜(ë‹¨ì–´) ì´ë¦„ ëª©ë¡.
        topic_components (np.array): LDA ëª¨ë¸ì˜ .components_ ì†ì„± (í† í”½-ë‹¨ì–´ ë¶„í¬ í–‰ë ¬).
        n_top_words (int): ê° í† í”½ì—ì„œ ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜.
    Returns:
        list of lists: ê° í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸.
    """
    top_keywords = []
    for topic_idx, topic in enumerate(topic_components):
        # topicì€ í•´ë‹¹ í† í”½ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ í¬í•¨í•˜ëŠ” NumPy ë°°ì—´ì…ë‹ˆë‹¤.
        # argsort()[-n_top_words-1:-1:-1]ëŠ” ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ìƒìœ„ n_top_words ê°œì˜ ì¸ë±ìŠ¤ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
        top_words_indices = topic.argsort()[:-n_top_words - 1:-1]
        
        # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ë‹¨ì–´ë“¤ì„ feature_namesì—ì„œ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
        keywords_for_topic = [feature_names[i] for i in top_words_indices]
        top_keywords.append(keywords_for_topic)
    return top_keywords

# --- Master Agentê°€ í˜¸ì¶œí•  Tool í•¨ìˆ˜ë“¤ ---
# from .utils import get_embedding_models, get_qdrant_client, get_openai_client, calculate_sentiment_score, summarize_text_for_topic # ğŸš¨ utils ì„í¬íŠ¸ í™•ì¸

# utils.pyì— ë‹¤ìŒ í•¨ìˆ˜ë“¤ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# - get_embedding_models
# - get_qdrant_client
# - get_openai_client
# - calculate_sentiment_score
# - summarize_text_for_topic

def run_ward_clustering(workspace, num_clusters=5):
    """
    ê³ ê°ì˜ ëª©ì†Œë¦¬(VOC) ë°ì´í„°ë¥¼ ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì£¼ìš” ì£¼ì œ ê·¸ë£¹ì„ ë°œê²¬í•˜ê³ ,
    ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    print(f"[CX Analysis] Running Ward Clustering with {num_clusters} clusters (ë™ê¸° ëª¨ë“œ)")

    retrieved_data = workspace["artifacts"].get("retrieved_data")
    if not retrieved_data:
        return {"error": "ë°ì´í„° í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ê²€ìƒ‰ëœ ë°ì´í„°ê°€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ê²€ìƒ‰ì„ í•´ì£¼ì„¸ìš”."}

    documents = [d.get('original_text', '') for d in retrieved_data.get('web_results', []) if d.get('original_text')]
    
    if not documents:
        return {"error": "í´ëŸ¬ìŠ¤í„°ë§í•  ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."}

    try:
        # 2. í…ìŠ¤íŠ¸ ë²¡í„°í™” (TF-IDF)
        vectorizer = TfidfVectorizer(max_features=2000, min_df=0.01, max_df=0.9)
        X = vectorizer.fit_transform(documents)

        if X.shape[1] == 0: # ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ì— ìœ íš¨í•œ í”¼ì²˜(ë‹¨ì–´)ê°€ ì—†ëŠ” ê²½ìš°
            return {"error": "TF-IDF ë²¡í„°í™” í›„ ìœ íš¨í•œ ë‹¨ì–´ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ê±°ë‚˜ TfidfVectorizer ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”."}

        # 3. K-Means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        if num_clusters > X.shape[0]:
            num_clusters = X.shape[0]
            print(f"âš ï¸ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ê°€ ë¬¸ì„œ ìˆ˜ë³´ë‹¤ ë§ì•„ {num_clusters}ê°œë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if num_clusters < 2:
            return {"error": "í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ëŠ” ìµœì†Œ 2ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."}

        kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        kmeans.fit(X)
        cluster_labels = kmeans.labels_.tolist() # ğŸš¨ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        # 4. ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ ì¶”ì¶œ
        feature_names = np.array(vectorizer.get_feature_names_out())
        cluster_centers = kmeans.cluster_centers_

        cluster_summaries = {}
        for i in range(num_clusters):
            cluster_docs_indices = np.where(kmeans.labels_ == i)[0]
            num_docs_in_cluster = len(cluster_docs_indices)

            if num_docs_in_cluster == 0:
                cluster_summaries[str(i)] = {"keywords": [], "description": f"{i}ë²ˆ ê·¸ë£¹ (0ê°œ ë¬¸ì„œ)ì—ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}
                continue

            cluster_tfidf_sum = X[cluster_docs_indices].sum(axis=0)
            top_feature_indices = cluster_tfidf_sum.A.flatten().argsort()[-10:][::-1]
            top_keywords = feature_names[top_feature_indices].tolist()

            cluster_summaries[str(i)] = {
                "keywords": top_keywords,
                "description": f"{i}ë²ˆ ê·¸ë£¹ ({num_docs_in_cluster}ê°œ ë¬¸ì„œ)ì€ ì£¼ë¡œ '{', '.join(top_keywords[:5])}...' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
            }
        
        # ğŸš¨ ì¶”ê°€: ì‹œê°í™”ë¥¼ ìœ„í•œ 2D ë°ì´í„° ì¶•ì†Œ (PCA)
        # ë°ì´í„° í¬ì¸íŠ¸ê°€ 2ê°œ ë¯¸ë§Œì¼ ê²½ìš° PCA ì˜¤ë¥˜ ë°©ì§€
        if X.shape[0] < 2:
            # ë°ì´í„° í¬ì¸íŠ¸ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ PCAë¥¼ ì ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë”ë¯¸ ë°ì´í„° ìƒì„± ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            reduced_features_2d = np.array([[0.0, 0.0]] * X.shape[0]).tolist() # ëª¨ë“  ì ì„ ì›ì ì—
        else:
            pca = PCA(n_components=2, random_state=42)
            # X (TF-IDF í–‰ë ¬)ëŠ” scipy.sparse.csr_matrix íƒ€ì…ì´ë¯€ë¡œ, toarray()ë¡œ ë³€í™˜í•˜ì—¬ PCAì— ì „ë‹¬
            reduced_features_2d = pca.fit_transform(X.toarray()).tolist() # ğŸš¨ numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        # 5. ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì„ì‹œ ë°ì´í„° ì €ì¥ (LDA, SNAë¥¼ ìœ„í•´)
        tfidf_matrix_list = X.toarray().tolist()
        feature_names_list = feature_names.tolist()

        cluster_docs_map = defaultdict(list)
        for doc_idx, label in enumerate(cluster_labels):
            cluster_docs_map[label].append(doc_idx)

        workspace["artifacts"]["_cx_temp_data"] = {
            "cluster_labels": cluster_labels,
            "tfidf_matrix": tfidf_matrix_list,
            "feature_names": feature_names_list,
            "documents": documents,
            "cluster_docs_map": dict(cluster_docs_map),
        }

        # 6. ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        return {
            "cx_ward_clustering_results": {
                "num_clusters": num_clusters,
                "cluster_labels": cluster_labels, # ğŸš¨ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ í¬í•¨
                "cluster_summaries": cluster_summaries,
                "visual_data": { # ğŸš¨ ì¶”ê°€: ì‹œê°í™” ë°ì´í„° í¬í•¨
                    "reduced_features_2d": reduced_features_2d,
                    "cluster_labels": cluster_labels,
                }
            },
            "analysis_results": "Ward clustering analysis complete. ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”. íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ ë” ê¹Šì€ ë¶„ì„(ì˜ë¯¸ ì—°ê²°ë§ ë¶„ì„)ì„ ì›í•˜ì‹œë©´ í´ëŸ¬ìŠ¤í„° IDì™€ í•¨ê»˜ ìš”ì²­í•´ì£¼ì„¸ìš”."
        }

    except Exception as e:
        print(f"âŒ ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}



def run_semantic_network_analysis(workspace: dict, cluster_id: int):
    """PDF 2ë‹¨ê³„: íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ SNAë¥¼ ìˆ˜í–‰í•˜ì—¬ í•µì‹¬ ë…¸ë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    print(f"âœ… [CX Agent] Step 2: Running SNA for Cluster ID: {cluster_id} (ë™ê¸° ëª¨ë“œ)")
    temp_data = workspace.get("artifacts", {}).get("_cx_temp_data", {})
    
    if not temp_data.get("cluster_labels"): return {"error": "êµ°ì§‘í™”ë¥¼ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."}
    if not temp_data.get("tfidf_matrix"): return {"error": "TF-IDF í–‰ë ¬ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}
    if not temp_data.get("feature_names"): return {"error": "í”¼ì²˜ ì´ë¦„ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}

    try:
        docs_indices = [i for i, label in enumerate(temp_data["cluster_labels"]) if label == cluster_id]
        if not docs_indices: return {"error": f"IDê°€ {cluster_id}ì¸ í´ëŸ¬ìŠ¤í„°ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        cluster_matrix = csr_matrix(np.array(temp_data["tfidf_matrix"])[docs_indices])

        co_occurrence_matrix = (cluster_matrix.T * cluster_matrix)
        co_occurrence_matrix.setdiag(0)

        G = nx.Graph()
        feature_names = temp_data["feature_names"]
        
        # ë…¸ë“œ ì¶”ê°€: ë…¸ë“œ IDë¥¼ ì‹¤ì œ í‚¤ì›Œë“œ ì´ë¦„ìœ¼ë¡œ ì§ì ‘ í• ë‹¹
        for i, name in enumerate(feature_names):
            G.add_node(name, id=name, name=name) # ğŸš¨ ë…¸ë“œ IDë¥¼ í‚¤ì›Œë“œ ì´ë¦„ìœ¼ë¡œ ëª…ì‹œ

        # ğŸš¨ ì—£ì§€ ì¶”ê°€: ë™ì‹œ ì¶œí˜„ ë¹ˆë„ ì„ê³„ê°’ ì¡°ì • (ë§¤ìš° ì¤‘ìš”!)
        # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë” ë§ì€ ì—£ì§€ ìƒì„±. ë°ì´í„°ì— ë”°ë¼ ì ì ˆí•œ ê°’ ì°¾ê¸°.
        threshold = 0.1 # ğŸš¨ ì´ˆê¸°ê°’ 2ì—ì„œ 1.0ìœ¼ë¡œ ì¡°ì • (ë” ë§ì€ ì—£ì§€ ìƒì„± ì‹œë„)
                        # ë§Œì•½ ì—¬ì „íˆ ê·¸ë˜í”„ê°€ ì•ˆ ë‚˜ì˜¤ë©´ 0.5, 0.1 ë“±ìœ¼ë¡œ ë” ë‚®ì¶°ë³´ì„¸ìš”.
                        # ì•„ì£¼ ë“œë¬¸ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ì—°ê²°ì„ ì‹œë„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
        
        # TF-IDF ê¸°ë°˜ ë™ì‹œ ì¶œí˜„ í–‰ë ¬ì˜ ê°’ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì—£ì§€ë¥¼ ì¶”ê°€
        # feature_namesì— ëŒ€í•œ ì¸ë±ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—£ì§€ë¥¼ ì¶”ê°€í•˜ê³ , ì‹¤ì œ í‚¤ì›Œë“œ ì´ë¦„ì€ ë…¸ë“œ ìì²´ì˜ 'id'ì™€ 'name' ì†ì„±ì„ í†µí•´ í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ë‹¬ë¨
        for i in range(co_occurrence_matrix.shape[0]):
            for j in range(i + 1, co_occurrence_matrix.shape[1]):
                weight = co_occurrence_matrix[i, j]
                if weight > threshold:
                    # ğŸš¨ ì—£ì§€ ì†ŒìŠ¤/íƒ€ê²Ÿì„ ì¸ë±ìŠ¤ ëŒ€ì‹  ì‹¤ì œ í‚¤ì›Œë“œ ì´ë¦„ìœ¼ë¡œ ë³€ê²½ (ë„¤íŠ¸ì›Œí¬X ì—£ì§€ëŠ” ë…¸ë“œ IDë¡œ ì—°ê²°)
                    G.add_edge(feature_names[i], feature_names[j], weight=float(weight)) # ğŸš¨ í‚¤ì›Œë“œ ì´ë¦„ìœ¼ë¡œ ì—£ì§€ ì—°ê²°

        # 4. í•µì‹¬ ë…¸ë“œ (í•µì‹¬ í‚¤ì›Œë“œ) ì¶”ì¶œ ë° Micro-segments, Graph Data êµ¬ì„±
        micro_segments = []
        graph_data = {} # ğŸš¨ graph_data ë³€ìˆ˜ë¥¼ ì—¬ê¸°ì„œ ì´ˆê¸°í™”

        # ì»¤ë®¤ë‹ˆí‹° ê°ì§€ ë° ê·¸ë˜í”„ ë°ì´í„° êµ¬ì„± (python-louvain í™œìš©)
        try:
            # import community as co # ì´ë¯¸ ìƒë‹¨ì— ì„í¬íŠ¸ë¨
            partitions = co.best_partition(G)
            
            for community_id in set(partitions.values()):
                community_nodes_names = [node_name for node_name, part_id in partitions.items() if part_id == community_id]
                
                if community_nodes_names:
                    # ì„œë¸Œê·¸ë˜í”„ëŠ” ì‹¤ì œ ë…¸ë“œ ì´ë¦„ìœ¼ë¡œ êµ¬ì„±
                    subgraph = G.subgraph(community_nodes_names)
                    if subgraph.nodes():
                        # í•´ë‹¹ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì¤‘ì‹¬ì„±ì´ ê°€ì¥ ë†’ì€ ë…¸ë“œë¥¼ í•µì‹¬ í‚¤ì›Œë“œë¡œ
                        sub_centrality = nx.degree_centrality(subgraph)
                        core_keyword_name = max(sub_centrality, key=sub_centrality.get)
                    else:
                        core_keyword_name = community_nodes_names[0]

                    micro_segments.append({
                        "community_id": community_id,
                        "core_keyword": core_keyword_name,
                        "keywords": community_nodes_names # ì»¤ë®¤ë‹ˆí‹°ì— ì†í•˜ëŠ” ëª¨ë“  í‚¤ì›Œë“œ (ì´ë¦„)
                    })
            micro_segments = sorted(micro_segments, key=lambda x: (x['community_id'], x['core_keyword']))

            # ğŸš¨ ìµœì¢… graph_data ìƒì„± ë° ë…¸ë“œì— ì»¤ë®¤ë‹ˆí‹° ì •ë³´ ì¶”ê°€
            graph_data_temp = nx.node_link_data(G)
            for node_data in graph_data_temp['nodes']:
                node_data['community'] = partitions.get(node_data['id'], None)
            centrality = nx.degree_centrality(G)    
            for node_data in graph_data_temp['nodes']:
                node_data['centrality'] = float(centrality.get(node_data['id'], 0.0))
            graph_data = graph_data_temp # ìµœì¢… graph_dataì— í• ë‹¹

        except ImportError:
            print("Warning: 'python-louvain' library not found. Community detection skipped. Micro-segments based on top keywords.")
            # python-louvainì´ ì—†ìœ¼ë©´ ì¤‘ì‹¬ì„± ë†’ì€ ìƒìœ„ Nê°œ í‚¤ì›Œë“œë¡œ êµ¬ì„±
            centrality = nx.degree_centrality(G)
            sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
            for node_name, score in sorted_nodes[:10]: # ë…¸ë“œ ì´ë¦„ê³¼ ì ìˆ˜
                micro_segments.append({
                    "community_id": "N/A",
                    "core_keyword": node_name, # ë…¸ë“œ ì´ë¦„ ì§ì ‘ ì‚¬ìš©
                    "keywords": [node_name],
                    "centrality_score": round(score, 4)
                })
            graph_data = nx.node_link_data(G) # Gì˜ ë…¸ë“œ IDëŠ” ì´ë¯¸ í‚¤ì›Œë“œ ì´ë¦„

        except Exception as e:
            print(f"Community detection or graph processing failed: {e}. Falling back to simple segments and basic graph data.")
            import traceback; traceback.print_exc() # ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
            micro_segments = [] # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
            
            centrality = nx.degree_centrality(G)
            sorted_nodes = sorted(centrality.items(), key=lambda x: x[1], reverse=True)
            for node_name, score in sorted_nodes[:min(len(sorted_nodes), 10)]: # ìµœëŒ€ 10ê°œ
                micro_segments.append({
                    "community_id": "Error",
                    "core_keyword": node_name,
                    "keywords": [node_name],
                    "centrality_score": round(score, 4)
                })
            graph_data = nx.node_link_data(G)

        # 5. ê²°ê³¼ ë°˜í™˜ (ì§ë ¬í™” ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ í¬í•¨)
        return {
            "cx_sna_results": {
                "cluster_id": cluster_id,
                "micro_segments": micro_segments,
                "graph_data": graph_data,
                "analysis_description": f"{cluster_id}ë²ˆ í´ëŸ¬ìŠ¤í„° ë‚´ì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ í‚¤ì›Œë“œë“¤ì„ ì°¾ì•„ ì˜ë¯¸ ì—°ê²°ë§ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤."
            },
            "analysis_results": "Semantic network analysis complete."
        }

    except Exception as e:
        print(f"âŒ SNA ë¶„ì„ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}") # ë©”ì‹œì§€ êµ¬ì²´í™”
        import traceback
        traceback.print_exc()
        return {"error": f"SNA ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}


def run_topic_modeling_lda(workspace: dict, cluster_id: int, num_topics: int = 3):
    """
    PDF 3ë‹¨ê³„: íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´ LDAë¥¼ ìˆ˜í–‰í•˜ì—¬ êµ¬ì²´ì ì¸ 'ê³ ê° ì•¡ì…˜'ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    """
    print(f"âœ… [CX Agent] Step 3: Running LDA for Cluster ID: {cluster_id} (ë™ê¸° ëª¨ë“œ)")
    artifacts = workspace.get("artifacts", {}) # artifactsë¥¼ ë¨¼ì € ê°€ì ¸ì˜µë‹ˆë‹¤.
    temp_data = artifacts.get("_cx_temp_data", {}) # _cx_temp_dataëŠ” artifacts ì•ˆì— ìˆìŠµë‹ˆë‹¤.
    
    # 1. í•„ìˆ˜ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬
    if not temp_data.get("cluster_labels"):
        return {"error": "í† í”½ ëª¨ë¸ë§ì„ ìœ„í•´ì„œëŠ” êµ°ì§‘í™”ë¥¼ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤."}
    if not temp_data.get("tfidf_matrix"):
        return {"error": "í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ TF-IDF í–‰ë ¬ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}
    if not temp_data.get("feature_names"):
        return {"error": "í† í”½ ëª¨ë¸ë§ì„ ìœ„í•œ í”¼ì²˜ ì´ë¦„ì´ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì—†ìŠµë‹ˆë‹¤."}

    try:
        # 2. íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë“¤ì˜ TF-IDF í–‰ë ¬ ì¶”ì¶œ
        docs_indices = [i for i, label in enumerate(temp_data["cluster_labels"]) if label == cluster_id]
        if not docs_indices:
            return {"error": f"IDê°€ {cluster_id}ì¸ í´ëŸ¬ìŠ¤í„°ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # _cx_temp_dataì— ì €ì¥ëœ tfidf_matrixëŠ” list of lists (Python ê°ì²´)ì´ë¯€ë¡œ
        # ë‹¤ì‹œ í¬ì†Œ í–‰ë ¬ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
        doc_term_matrix = csr_matrix(np.array(temp_data["tfidf_matrix"])[docs_indices])
        
        # 3. í”¼ì²˜ ì´ë¦„(ë‹¨ì–´ ëª©ë¡) ê°€ì ¸ì˜¤ê¸°
        feature_names = temp_data["feature_names"] # TfidfVectorizer ê°ì²´ ëŒ€ì‹  ì €ì¥ëœ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©

        # ğŸš¨ ì¶”ê°€: LDA ëª¨ë¸ í•™ìŠµ ì „ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        if doc_term_matrix.shape[0] < num_topics:
            return {"error": f"ë¬¸ì„œ ê°œìˆ˜({doc_term_matrix.shape[0]}ê°œ)ê°€ í† í”½ ê°œìˆ˜({num_topics})ë³´ë‹¤ ì ìŠµë‹ˆë‹¤. ë” ì‘ì€ í† í”½ ê°œìˆ˜ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}
        if doc_term_matrix.shape[1] == 0: # ìœ íš¨í•œ ë‹¨ì–´ê°€ ì—†ìœ¼ë©´
            return {"error": "í† í”½ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. CountVectorizer ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”."}

        # 4. LDA ëª¨ë¸ í•™ìŠµ
        lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
        lda.fit(doc_term_matrix) # ë¬¸ì„œ-ë‹¨ì–´ í–‰ë ¬ë¡œ LDA í•™ìŠµ

        # ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬ë¥¼ ê³„ì‚° (calculate_opportunity_scoresì—ì„œ ì‚¬ìš©ë  ë°ì´í„°)
        doc_topic_dist_for_cluster = lda.transform(doc_term_matrix) # ğŸ‘ˆ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë“¤ì˜ í† í”½ ë¶„í¬
        assignments = np.argmax(doc_topic_dist_for_cluster, axis=1)
        # 5. í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ìš”ì•½ ì •ë³´ êµ¬ì„± (ê¸°ì¡´ ë¡œì§)
        topics_list = []
        top_keywords_per_topic = _get_top_keywords(feature_names, lda.components_, 7) # ìƒìœ„ 7ê°œ í‚¤ì›Œë“œ

        # ğŸš¨ LDA ê·¸ë˜í”„ ì‹œê°í™” ë°ì´í„° ìƒì„± ì‹œì‘ ğŸš¨
        topic_graph_data_points = [] # ê° í† í”½ì˜ 2D ìœ„ì¹˜ (ê·¸ë˜í”„ ì )
        topic_keywords_with_weights = [] # ê° í† í”½ì˜ í‚¤ì›Œë“œ ë° ê°€ì¤‘ì¹˜ (íˆ´íŒìš©)

        # LDA ëª¨ë¸ì˜ components_ëŠ” (num_topics, num_features) í˜•íƒœì˜ í† í”½-ë‹¨ì–´ ë¶„í¬ í–‰ë ¬
        topic_embeddings = lda.components_

        # PCAë¥¼ ì‚¬ìš©í•˜ì—¬ í† í”½ ì„ë² ë”©ì„ 2Dë¡œ ì¶•ì†Œ
        # í† í”½ ê°œìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì´ê±°ë‚˜, í”¼ì²˜ ê°œìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ PCA ì ìš© ë¶ˆê°€
        if topic_embeddings.shape[0] >= 2 and topic_embeddings.shape[1] >= 2:
            pca = PCA(n_components=2, random_state=42)
            # ê° í† í”½ì˜ 2D ìœ„ì¹˜
            topic_positions_2d = pca.fit_transform(topic_embeddings).tolist()
        else: # PCA ì ìš© ë¶ˆê°€ ì‹œ ì„ì‹œ ìœ„ì¹˜ í• ë‹¹
            topic_positions_2d = [[np.random.rand() * 10, np.random.rand() * 10] for _ in range(num_topics)]
            print("Warning: Not enough data for meaningful PCA for LDA topics. Using random positions for graph.")


        for i, keywords in enumerate(top_keywords_per_topic):
            topic_docs = [
                docs_indices[j]
                for j, assigned in enumerate(assignments)
                if assigned == i
            ]
            # í† í”½ë³„ ìƒìœ„ í‚¤ì›Œë“œì˜ ê°€ì¤‘ì¹˜ë„ í•¨ê»˜ ì¶”ì¶œ (í™•ë¥ ë¡œ ì •ê·œí™”)
            current_topic_comp = lda.components_[i]
            keywords_and_weights = {
                kw: float(current_topic_comp[feature_names.index(kw)]) / current_topic_comp.sum()
                for kw in keywords if kw in feature_names
            }
            topic_keywords_with_weights.append({
                "topic_id": i,
                "keywords": keywords_and_weights
            })

            # LDA ê·¸ë˜í”„ì— í‘œì‹œë  ë°ì´í„° í¬ì¸íŠ¸
            topic_graph_data_points.append({
                "topic_id": i,
                "x": topic_positions_2d[i][0],
                "y": topic_positions_2d[i][1],
                "keywords_data": keywords_and_weights # í•´ë‹¹ í† í”½ì˜ í‚¤ì›Œë“œ ë°ì´í„° (íˆ´íŒìš©)
            })

            # ê¸°ì¡´ topics_list (ìš”ì•½ ë©”ì‹œì§€ìš©)
            topics_list.append({
                "topic_id": f"{cluster_id}-{i}", # í´ëŸ¬ìŠ¤í„° IDì™€ í† í”½ ì¸ë±ìŠ¤ë¥¼ ì¡°í•©
                "action_keywords": keywords, # ìƒìœ„ í‚¤ì›Œë“œ
                "description": f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:5])}...", # ê°„ë‹¨í•œ ì„¤ëª… ì¶”ê°€
                "document_indices": topic_docs
            })

        # ğŸš¨ LDA ê·¸ë˜í”„ ì‹œê°í™” ë°ì´í„° ìµœì¢… êµ¬ì„±
        lda_graph_data = {
            "topics": topic_graph_data_points,
            "num_topics": num_topics
      
        }
     

        # 6. ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì„ì‹œ ë°ì´í„° ë° LDA ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ ë¡œì§) ëª°ë¼ ì¼ë‹¨ ì‚´ë¦¬ì.
        # workspace["artifacts"]["_cx_temp_data"]["doc_topic_distribution"] = doc_topic_dist_for_cluster.tolist()
        # print(doc_topic_dist_for_cluster.tolist())
        # workspace["artifacts"]["_cx_temp_data"]["lda_components_for_cluster"] = lda.components_.tolist()

        # 7. LLMì—ê²Œ ë°˜í™˜í•  ë°ì´í„° (ê°„ê²°í•˜ê²Œ)
        return {
            "cx_lda_results": { # ArtifactRendererì—ì„œ ì´ í‚¤ë¥¼ í†µí•´ ì ‘ê·¼
                "cluster_id": cluster_id,
                "num_topics": num_topics,
                "topics_summary_list": topics_list, # ê¸°ì¡´ í† í”½ ìš”ì•½ ë©”ì‹œì§€ìš© ë¦¬ìŠ¤íŠ¸
                "graph_data": lda_graph_data # ğŸš¨ ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ê·¸ë˜í”„ ë°ì´í„°
            },
            "success": True, # ì„±ê³µ ì—¬ë¶€
            "message": f"í´ëŸ¬ìŠ¤í„° {cluster_id}ì— ëŒ€í•´ {num_topics}ê°œì˜ í† í”½ì„ ì„±ê³µì ìœ¼ë¡œ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.",
            "newly_identified_topics_preview": [ # LLM ë©”ì‹œì§€ìš© ê°„ëµí™”ëœ ë¯¸ë¦¬ë³´ê¸°
                {"topic_id": t["topic_id"], "action_keywords": t["action_keywords"]}
                for t in topics_list
            ]
        }

    except Exception as e:
        print(f"âŒ í† í”½ ëª¨ë¸ë§(LDA) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"í† í”½ ëª¨ë¸ë§(LDA) ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

def create_customer_action_map(workspace: dict, topic_id: str):
    """
    [ì™„ì„±ë³¸] PDF 4ë‹¨ê³„: 'ë¶„ì„ëœ ê²°ê³¼'ë¥¼ ë°”íƒ•ìœ¼ë¡œ CAM(Pain Point ë“±)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print(f"âœ… [CX Agent] Step 4: Creating CAM for Topic ID: {topic_id}...")
    client = get_openai_client()
    
    # --- 1. workspaceì—ì„œ ì´ í† í”½ì— ëŒ€í•œ ëª¨ë“  ë¶„ì„ 'ê²°ê³¼'ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ---
    artifacts = workspace.get("artifacts", {})
    #lda_results = artifacts.get("cx_lda_results", [])
    lda_results = artifacts.get("cx_lda_results", {}).get("topics_summary_list", [])
    opportunity_scores = artifacts.get("cx_opportunity_scores", [])
    
    # í•´ë‹¹ topic_idì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    topic_lda_data = next((item for item in lda_results if item.get("topic_id") == topic_id), None)
    topic_score_data = next((item for item in opportunity_scores if item.get("topic_id") == topic_id), None)

    if not topic_lda_data or not topic_score_data:
        return {"error": f"IDê°€ {topic_id}ì¸ í† í”½ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. LDAì™€ ê¸°íšŒ ì ìˆ˜ ê³„ì‚°ì„ ë¨¼ì € ìˆ˜í–‰í•´ì£¼ì„¸ìš”."}
    
        # --- 1.5. graph_dataì—ì„œ keywords_data ì¶”ì¶œ ---
    graph_topics = artifacts.get("cx_lda_results", {}).get("graph_data", {}).get("topics", [])
    # topic_idëŠ” "cluster-topicIndex" í˜•íƒœì´ë‹ˆ, ë ìˆ«ìë§Œ íŒŒì‹±
    try:
        idx = int(topic_id.split("-")[-1])
    except ValueError:
        idx = None
    keywords_data = {}
    if idx is not None:
        tg = next((t for t in graph_topics if t.get("topic_id")==idx), None)
        if tg:
            keywords_data = tg.get("keywords_data", {})



    action_keywords = topic_lda_data.get('action_keywords', [])
    first_keyword = action_keywords[0] if action_keywords else topic_id # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ topic_id ì‚¬ìš©

    # --- 2. LLMì—ê²Œ ì „ë‹¬í•  'ë¶„ì„ ìš”ì•½ ì •ë³´'ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ---
    prompt = f"""
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ì—¬ ê³ ê° ì•¡ì…˜ë§µ(CAM)ì„ ì™„ì„±í•˜ëŠ” ìµœê³ ì˜ CX ì „ëµê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” íŠ¹ì • ê³ ê° í–‰ë™(Action)ì— ëŒ€í•œ ì •ëŸ‰ì /ì •ì„±ì  ë¶„ì„ ìš”ì•½ ê²°ê³¼ì…ë‹ˆë‹¤.

    [ë¶„ì„ ë°ì´í„° ìš”ì•½]
    - í–‰ë™(Action) ID: {topic_id}
    - í–‰ë™ì˜ í•µì‹¬ í‚¤ì›Œë“œ: "{', '.join(topic_lda_data.get('action_keywords', []))}"
    - ì´ í–‰ë™ì— ëŒ€í•œ ê³ ê° ë§Œì¡±ë„ ì ìˆ˜: {topic_score_data.get('satisfaction')} (-1.0: ë§¤ìš° ë¶€ì •, 1.0: ë§¤ìš° ê¸ì •)
    - ì´ í–‰ë™ì˜ ì¤‘ìš”ë„(ì–¸ê¸‰ëŸ‰): {topic_score_data.get('importance')}

    ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ í–‰ë™ì„ í•˜ëŠ” ê³ ê°ë“¤ì˜ 'Goal(ê¶ê·¹ì  ëª©í‘œ)'ê³¼ 'Pain Point(í•µì‹¬ ë¶ˆí¸í•¨)'ë¥¼ ê°ê° 2~3ê°€ì§€ì”© ê¹Šì´ ìˆê²Œ ì¶”ë¡ í•´ì£¼ì„¸ìš”.
    PDFì˜ CAM í”„ë ˆì„ì›Œí¬ë¥¼ ì°¸ê³ í•˜ì—¬, ì´ í–‰ë™ì´ ì£¼ë¡œ ë°œìƒí•˜ëŠ” 'Context(ìƒí™©)'ì™€ ê´€ë ¨ëœ 'Touchpoint/Artifact(ì‚¬ë¬¼/ì„œë¹„ìŠ¤)'ë„ í•¨ê»˜ ì¶”ë¡ í•˜ì—¬ ì œì‹œí•´ì£¼ì„¸ìš”.

    ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
    {{
      "action_name": "{first_keyword}",
      "goals": ["ì¶”ë¡ ëœ ëª©í‘œ 1", "ì¶”ë¡ ëœ ëª©í‘œ 2"],
      "pain_points": ["ì¶”ë¡ ëœ ë¶ˆí¸í•¨ 1", "ì¶”ë¡ ëœ ë¶ˆí¸í•¨ 2"],
      "context": ["ì¶”ë¡ ëœ ìƒí™© 1", "ì¶”ë¡ ëœ ìƒí™© 2"],
      "touchpoint_artifact": ["ê´€ë ¨ëœ ì‚¬ë¬¼ 1", "ê´€ë ¨ëœ ì‚¬ë¬¼ 2"]
    }}
    """
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        cam_results = json.loads(res.choices[0].message.content)
        cam_results["keywords_data"] = keywords_data
        existing_cams = workspace.get("artifacts", {}).get("cx_cam_results", [])
        existing_cams.append(cam_results)
        
        return {"cx_cam_results": existing_cams}
    except Exception as e:
        return {"error": f"ê³ ê° ì•¡ì…˜ë§µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"}


def calculate_opportunity_scores(workspace):
    # 1) LDA í† í”½ ë¶„ì„ ê²°ê³¼ & ì›ë¬¸ êº¼ë‚´ì˜¤ê¸°
    lda_results = workspace["artifacts"]["cx_lda_results"]["topics_summary_list"]
    all_docs    = [d["original_text"] for d in workspace["artifacts"]["retrieved_data"]["web_results"]]

    # 2) ì›ì‹œ ì¤‘ìš”ë„Â·ê°ì„± ìˆ˜ì§‘
    raw_importances, raw_sentiments = [], []
    for topic in lda_results:
        idxs = topic.get("document_indices", [])
        docs = [all_docs[i] for i in idxs if i < len(all_docs)]
        raw_importances.append(len(docs))
        scores = [_get_sentiment_score(doc) for doc in docs]
        raw_sentiments.append(float(np.mean(scores)) if scores else 0.0)

    # 3) ì •ê·œí™”
    imp_min, imp_max = min(raw_importances), max(raw_importances)
    sent_min, sent_max = min(raw_sentiments), max(raw_sentiments)
    def norm(x, lo, hi): return 0.0 if hi==lo else (x-lo)/(hi-lo)*10
    norm_imps = [norm(v, imp_min, imp_max) for v in raw_importances]
    norm_sats = [norm(v, sent_min, sent_max) for v in raw_sentiments]

    # 4) ê¸°íšŒ ì ìˆ˜ ê³„ì‚°
    opportunity_scores = []
    for i, topic in enumerate(lda_results):
        imp10 = norm_imps[i]
        sat10 = norm_sats[i]
        opp   = imp10 + (10 - sat10)  # ë§ì…ˆ ëª¨ë¸
        opportunity_scores.append({
            "topic_id":          topic["topic_id"],
            "action_keywords":   topic["action_keywords"],
            "importance":        round(imp10, 2),
            "satisfaction":      round(sat10, 2),
            "opportunity_score": round(opp, 2)
        })

    # 5) ì •ë ¬ & ë°˜í™˜
    sorted_scores = sorted(opportunity_scores, key=lambda x: x["opportunity_score"], reverse=True)
    workspace["artifacts"]["cx_opportunity_scores"] = sorted_scores
    return {"cx_opportunity_scores": sorted_scores}
